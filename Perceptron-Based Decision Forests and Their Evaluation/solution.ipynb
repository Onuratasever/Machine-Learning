{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "55341b86",
      "metadata": {
        "id": "55341b86"
      },
      "source": [
        "The Breast Cancer dataset was selected for this project because it is a well-established, relatively balanced binary classification dataset, making it ideal for evaluating classification models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "c7fb18f8",
      "metadata": {
        "id": "c7fb18f8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "np.random.seed(42)  # For reproducibility\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Dataset"
      ],
      "metadata": {
        "id": "hflITtSc-5ts"
      },
      "id": "hflITtSc-5ts"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "50ec3e2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50ec3e2f",
        "outputId": "d1a57a0a-4b85-4808-e0ea-637222374541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Part 1: Data Analysis and Preparation ===\n",
            "Train shape: (455, 30), Test shape: (114, 30)\n",
            "\n",
            "\n",
            "=== Training Set Analysis (Scaled Features) ===\n",
            "                 0             1             2             3             4   \\\n",
            "count  4.550000e+02  4.550000e+02  4.550000e+02  4.550000e+02  4.550000e+02   \n",
            "mean  -4.337434e-15  2.240942e-15 -7.437274e-16  1.503071e-16  5.223660e-15   \n",
            "std    1.001101e+00  1.001101e+00  1.001101e+00  1.001101e+00  1.001101e+00   \n",
            "min   -2.027220e+00 -2.167362e+00 -1.980187e+00 -1.465734e+00 -2.503730e+00   \n",
            "25%   -6.958063e-01 -7.379620e-01 -6.979907e-01 -6.758983e-01 -7.210063e-01   \n",
            "50%   -2.280663e-01 -9.711800e-02 -2.312101e-01 -3.097848e-01 -3.774691e-02   \n",
            "75%    4.785500e-01  5.596334e-01  5.033684e-01  3.526440e-01  6.458845e-01   \n",
            "max    4.017353e+00  4.552410e+00  4.018733e+00  5.370416e+00  3.610271e+00   \n",
            "\n",
            "                 5             6             7             8             9   \\\n",
            "count  4.550000e+02  4.550000e+02  4.550000e+02  4.550000e+02  4.550000e+02   \n",
            "mean  -2.775802e-15 -7.046866e-16  6.031805e-16 -3.263812e-15 -3.031519e-15   \n",
            "std    1.001101e+00  1.001101e+00  1.001101e+00  1.001101e+00  1.001101e+00   \n",
            "min   -1.580330e+00 -1.092292e+00 -1.243358e+00 -2.660791e+00 -1.798182e+00   \n",
            "25%   -7.504076e-01 -7.492716e-01 -7.233200e-01 -6.728116e-01 -7.179958e-01   \n",
            "50%   -2.412686e-01 -3.576338e-01 -4.041472e-01 -8.978972e-02 -1.929345e-01   \n",
            "75%    4.906419e-01  5.266743e-01  6.552186e-01  5.235422e-01  4.796473e-01   \n",
            "max    4.517740e+00  4.134445e+00  3.931305e+00  4.399657e+00  4.840942e+00   \n",
            "\n",
            "       ...            20            21            22            23  \\\n",
            "count  ...  4.550000e+02  4.550000e+02  4.550000e+02  4.550000e+02   \n",
            "mean   ...  4.060244e-16  5.676534e-15 -2.402962e-15 -1.022869e-15   \n",
            "std    ...  1.001101e+00  1.001101e+00  1.001101e+00  1.001101e+00   \n",
            "min    ... -1.730874e+00 -2.191368e+00 -1.695348e+00 -1.238101e+00   \n",
            "25%    ... -6.647169e-01 -7.328464e-01 -6.909317e-01 -6.429737e-01   \n",
            "50%    ... -2.659572e-01 -3.976710e-02 -2.724877e-01 -3.360859e-01   \n",
            "75%    ...  4.979823e-01  5.986377e-01  5.526388e-01  2.977870e-01   \n",
            "max    ...  3.557938e+00  3.842120e+00  3.699640e+00  4.640386e+00   \n",
            "\n",
            "                 24            25            26            27            28  \\\n",
            "count  4.550000e+02  4.550000e+02  4.550000e+02  4.550000e+02  4.550000e+02   \n",
            "mean   4.172487e-15 -5.465713e-16 -6.383172e-16 -4.489693e-16 -1.034581e-15   \n",
            "std    1.001101e+00  1.001101e+00  1.001101e+00  1.001101e+00  1.001101e+00   \n",
            "min   -2.715107e+00 -1.421602e+00 -1.282241e+00 -1.707442e+00 -2.081459e+00   \n",
            "25%   -7.271308e-01 -6.792384e-01 -7.820068e-01 -7.572169e-01 -6.423407e-01   \n",
            "50%   -4.123889e-02 -2.675449e-01 -2.168678e-01 -2.501005e-01 -1.388815e-01   \n",
            "75%    6.157967e-01  5.163947e-01  5.040438e-01  7.284559e-01  4.528766e-01   \n",
            "max    3.821065e+00  5.032188e+00  4.522140e+00  2.654689e+00  5.777151e+00   \n",
            "\n",
            "                 29  \n",
            "count  4.550000e+02  \n",
            "mean  -2.088683e-15  \n",
            "std    1.001101e+00  \n",
            "min   -1.571972e+00  \n",
            "25%   -6.585793e-01  \n",
            "50%   -2.183342e-01  \n",
            "75%    4.418976e-01  \n",
            "max    6.719538e+00  \n",
            "\n",
            "[8 rows x 30 columns]\n",
            "\n",
            "=== Test Set Analysis (Scaled Features) ===\n",
            "               0           1           2           3           4           5   \\\n",
            "count  114.000000  114.000000  114.000000  114.000000  114.000000  114.000000   \n",
            "mean     0.085785    0.047963    0.085170    0.091955    0.071604    0.044073   \n",
            "std      1.037502    0.879544    1.033388    1.099944    1.212915    0.936451   \n",
            "min     -1.804364   -1.653778   -1.808978   -1.356901   -3.238689   -1.359990   \n",
            "25%     -0.622856   -0.582296   -0.629383   -0.621917   -0.802648   -0.687139   \n",
            "50%     -0.110774   -0.050532   -0.151202   -0.223734    0.052263   -0.116883   \n",
            "75%      0.623020    0.607924    0.620478    0.471635    0.843014    0.517296   \n",
            "max      3.819958    2.291276    3.952405    5.376220    5.001338    3.238346   \n",
            "\n",
            "               6           7           8           9   ...          20  \\\n",
            "count  114.000000  114.000000  114.000000  114.000000  ...  114.000000   \n",
            "mean    -0.024083    0.073837    0.096794   -0.015473  ...    0.096334   \n",
            "std      0.873990    0.987532    0.877151    0.934047  ...    1.066774   \n",
            "min     -1.092292   -1.243358   -1.615988   -1.731063  ...   -1.410607   \n",
            "25%     -0.613582   -0.669953   -0.583665   -0.653324  ...   -0.659470   \n",
            "50%     -0.272522   -0.172162    0.133078   -0.069185  ...   -0.247069   \n",
            "75%      0.345122    0.635994    0.597534    0.409732  ...    0.695263   \n",
            "max      3.359252    3.586671    2.812839    4.502553  ...    4.168670   \n",
            "\n",
            "               21          22          23          24          25          26  \\\n",
            "count  114.000000  114.000000  114.000000  114.000000  114.000000  114.000000   \n",
            "mean     0.024020    0.095722    0.104435    0.008790   -0.001985   -0.101577   \n",
            "std      0.939801    1.062129    1.142849    1.066720    0.923005    0.816912   \n",
            "min     -1.856889   -1.431465   -1.087101   -1.939539   -1.264876   -1.282241   \n",
            "25%     -0.743701   -0.649917   -0.639262   -0.546224   -0.662019   -0.719420   \n",
            "50%     -0.034943   -0.236298   -0.318161   -0.050118   -0.229350   -0.305649   \n",
            "75%      0.760651    0.657438    0.499663    0.582501    0.551146    0.460001   \n",
            "max      2.605512    4.360103    6.128657    4.007521    3.843129    3.069654   \n",
            "\n",
            "               27          28          29  \n",
            "count  114.000000  114.000000  114.000000  \n",
            "mean     0.052515   -0.061059    0.000326  \n",
            "std      0.922739    0.766424    0.907320  \n",
            "min     -1.707442   -1.409148   -1.338662  \n",
            "25%     -0.600383   -0.584249   -0.694201  \n",
            "50%     -0.065272   -0.133460   -0.185431  \n",
            "75%      0.646010    0.257302    0.447064  \n",
            "max      2.321908    2.857088    3.217155  \n",
            "\n",
            "[8 rows x 30 columns]\n",
            "\n",
            "=== Class Distribution in Training Set ===\n",
            "Class -1: 170 samples\n",
            "Class 1: 285 samples\n",
            "\n",
            "=== Class Distribution in Test Set ===\n",
            "Class -1: 42 samples\n",
            "Class 1: 72 samples\n"
          ]
        }
      ],
      "source": [
        "# Loading the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Convert labels {0,1} → {-1, +1} for AdaBoost math\n",
        "y_signed = np.where(y == 0, -1, 1)\n",
        "\n",
        "# Train/test split 80 / 20\n",
        "X_train, X_test, y_train_signed, y_test_signed = train_test_split(\n",
        "    X, y_signed, test_size=0.2, stratify=y_signed, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Displaying data analysis results in the terminal\n",
        "print(\"\\n=== Part 1: Data Analysis and Preparation ===\")\n",
        "print(f\"Train shape: {X_train_scaled.shape}, Test shape: {X_test_scaled.shape}\\n\")\n",
        "\n",
        "# Analyzing the data distribution (Descriptive Statistics)\n",
        "train_df = pd.DataFrame(X_train_scaled)\n",
        "test_df = pd.DataFrame(X_test_scaled)\n",
        "\n",
        "print(\"\\n=== Training Set Analysis (Scaled Features) ===\")\n",
        "print(train_df.describe())\n",
        "\n",
        "print(\"\\n=== Test Set Analysis (Scaled Features) ===\")\n",
        "print(test_df.describe())\n",
        "\n",
        "# Class Distribution in Train and Test Sets\n",
        "unique_train, counts_train = np.unique(y_train_signed, return_counts=True)\n",
        "unique_test, counts_test = np.unique(y_test_signed, return_counts=True)\n",
        "\n",
        "print(\"\\n=== Class Distribution in Training Set ===\")\n",
        "for label, count in zip(unique_train, counts_train):\n",
        "    print(f\"Class {label}: {count} samples\")\n",
        "\n",
        "print(\"\\n=== Class Distribution in Test Set ===\")\n",
        "for label, count in zip(unique_test, counts_test):\n",
        "    print(f\"Class {label}: {count} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLP Implementation"
      ],
      "metadata": {
        "id": "Eosuu8Te_Jsx"
      },
      "id": "Eosuu8Te_Jsx"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "d9050059",
      "metadata": {
        "id": "d9050059"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    \"\"\"One‑hidden‑layer neural network with tanh hidden units and softmax output.\n",
        "    Supports sample weights for AdaBoost training.\"\"\"\n",
        "    def __init__(self, n_in, n_hidden, n_out, lr=0.01, epochs=200):\n",
        "        self.W1 = np.random.randn(n_in, n_hidden) * np.sqrt(2.0 / n_in)\n",
        "        self.b1 = np.zeros((1, n_hidden))\n",
        "        self.W2 = np.random.randn(n_hidden, n_out) * np.sqrt(2.0 / n_hidden)\n",
        "        self.b2 = np.zeros((1, n_out))\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "    @staticmethod\n",
        "    def _softmax(z):\n",
        "        expz = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return expz / np.sum(expz, axis=1, keepdims=True)\n",
        "\n",
        "    def _forward(self, X):\n",
        "        z1 = X @ self.W1 + self.b1\n",
        "        a1 = np.tanh(z1)\n",
        "        z2 = a1 @ self.W2 + self.b2\n",
        "        a2 = self._softmax(z2)\n",
        "        return z1, a1, z2, a2\n",
        "\n",
        "    def _backward(self, X, y_onehot, sample_weights, cache):\n",
        "        z1, a1, _, a2 = cache\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Weighted cross‑entropy gradient\n",
        "        delta2 = (a2 - y_onehot) * sample_weights[:, None] / m\n",
        "        dW2 = a1.T @ delta2\n",
        "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "\n",
        "        delta1 = (1 - np.tanh(z1) ** 2) * (delta2 @ self.W2.T)\n",
        "        dW1 = X.T @ delta1\n",
        "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        # Parameter update (SGD on full batch)\n",
        "        self.W2 -= self.lr * dW2\n",
        "        self.b2 -= self.lr * db2\n",
        "        self.W1 -= self.lr * dW1\n",
        "        self.b1 -= self.lr * db1\n",
        "\n",
        "    def fit(self, X, y_signed, sample_weights=None):\n",
        "        if sample_weights is None:\n",
        "            sample_weights = np.ones(X.shape[0]) / X.shape[0]\n",
        "        # Binary classification → two output neurons (‑1 and +1)\n",
        "        y_idx = (y_signed == 1).astype(int)          # map {‑1:0, +1:1}\n",
        "        y_onehot = np.eye(2)[y_idx]\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            cache = self._forward(X)\n",
        "            self._backward(X, y_onehot, sample_weights, cache)\n",
        "\n",
        "    def predict(self, X):\n",
        "        _, _, _, probs = self._forward(X)\n",
        "        pred_idx = np.argmax(probs, axis=1)\n",
        "        return np.where(pred_idx == 1, 1, -1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AdaBoost using MLP weak learner"
      ],
      "metadata": {
        "id": "qkQbjzFB_UUO"
      },
      "id": "qkQbjzFB_UUO"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "d2862fdc",
      "metadata": {
        "id": "d2862fdc"
      },
      "outputs": [],
      "source": [
        "class AdaBoostMLP:\n",
        "    \"\"\"AdaBoost one‑hidden‑layer MLPs as weak learners\"\"\"\n",
        "    def __init__(self, n_estimators=10, hidden_size=4, lr=0.03, epochs=30):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.alphas = []\n",
        "        self.models = []\n",
        "\n",
        "    def fit(self, X, y_signed):\n",
        "        n_samples, n_features = X.shape\n",
        "        # Initialize uniform distribution\n",
        "        w = np.ones(n_samples) / n_samples\n",
        "\n",
        "        for t in range(self.n_estimators):\n",
        "            # Train weak learner\n",
        "            model = MLP(n_features, self.hidden_size, 2, lr=self.lr, epochs=self.epochs)\n",
        "            model.fit(X, y_signed, sample_weights=w)\n",
        "            pred = model.predict(X)\n",
        "\n",
        "            # Compute weighted error\n",
        "            incorrect = (pred != y_signed).astype(float)\n",
        "            err = np.dot(w, incorrect) / np.sum(w)\n",
        "\n",
        "            # Guard against perfect / worse‑than‑random learners\n",
        "            err = np.clip(err, 1e-10, 0.999)\n",
        "            alpha = 0.5 * np.log((1 - err) / err)\n",
        "\n",
        "            # Update sample weights\n",
        "            w *= np.exp(-alpha * y_signed * pred)\n",
        "            w /= np.sum(w)  # normalize\n",
        "\n",
        "            # Save\n",
        "            self.alphas.append(alpha)\n",
        "            self.models.append(model)\n",
        "\n",
        "            # print(f\"Iter {t+1}/{self.n_estimators} – error: {err:.4f}, alpha: {alpha:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Weighted majority vote\n",
        "        agg = np.zeros(X.shape[0])\n",
        "        for alpha, model in zip(self.alphas, self.models):\n",
        "            agg += alpha * model.predict(X)\n",
        "        return np.sign(agg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Single MLP"
      ],
      "metadata": {
        "id": "ktvqDi5I_ZTY"
      },
      "id": "ktvqDi5I_ZTY"
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "cd4c0a15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd4c0a15",
        "outputId": "ef05175a-102d-4f54-8337-ec4164438e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Validation Baseline MLP (5-Fold)\n",
            "Mean CV Accuracy: 0.4920\n",
            "All CV Accuracies: [0.45614035087719296, 0.4824561403508772, 0.7105263157894737, 0.39473684210526316, 0.415929203539823]\n"
          ]
        }
      ],
      "source": [
        "# Cross-Validation (5-Fold) Setup\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_accuracies = []\n",
        "\n",
        "# Cross-Validation Loop for Baseline MLP\n",
        "for train_index, test_index in cv.split(X, y_signed):\n",
        "    X_cv_train, X_cv_test = X[train_index], X[test_index]\n",
        "    y_cv_train, y_cv_test = y_signed[train_index], y_signed[test_index]\n",
        "\n",
        "    # Scaling within each fold\n",
        "    scaler = StandardScaler()\n",
        "    X_cv_train = scaler.fit_transform(X_cv_train)\n",
        "    X_cv_test = scaler.transform(X_cv_test)\n",
        "\n",
        "    # Baseline MLP Model Initialization\n",
        "    baseline_mlp = MLP(X_cv_train.shape[1], n_hidden=10, n_out=2, lr=0.02, epochs=300)\n",
        "    baseline_mlp.fit(X_cv_train, y_cv_train)\n",
        "\n",
        "    # Prediction on CV Test Set\n",
        "    y_pred_base = baseline_mlp.predict(X_cv_test)\n",
        "\n",
        "    # Calculating and Storing CV Accuracy\n",
        "    cv_accuracy = accuracy_score(y_cv_test, y_pred_base)\n",
        "    cv_accuracies.append(cv_accuracy)\n",
        "\n",
        "# Calculating Average CV Accuracy\n",
        "mean_cv_accuracy = np.mean(cv_accuracies)\n",
        "\n",
        "print(\"\\nCross-Validation Baseline MLP (5-Fold)\")\n",
        "print(f\"Mean CV Accuracy: {mean_cv_accuracy:.4f}\")\n",
        "print(f\"All CV Accuracies: {cv_accuracies}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AdaBoost with MLP weak learners"
      ],
      "metadata": {
        "id": "Df_JX6pg_jeM"
      },
      "id": "Df_JX6pg_jeM"
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "38ea757b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38ea757b",
        "outputId": "06a876ce-64d7-4356-bc53-d932fb3a406e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Validation AdaBoost-MLP Ensemble (5-Fold)\n",
            "Mean CV Accuracy: 0.9174\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.90      0.89       212\n",
            "           1       0.94      0.93      0.93       357\n",
            "\n",
            "    accuracy                           0.92       569\n",
            "   macro avg       0.91      0.91      0.91       569\n",
            "weighted avg       0.92      0.92      0.92       569\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cross-Validation Setup\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_accuracies = []\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "\n",
        "# Cross-Validation Loop\n",
        "for train_index, test_index in cv.split(X, y_signed):\n",
        "    X_cv_train, X_cv_test = X[train_index], X[test_index]\n",
        "    y_cv_train, y_cv_test = y_signed[train_index], y_signed[test_index]\n",
        "\n",
        "    # Scaling within each fold\n",
        "    scaler = StandardScaler()\n",
        "    X_cv_train = scaler.fit_transform(X_cv_train)\n",
        "    X_cv_test = scaler.transform(X_cv_test)\n",
        "\n",
        "    # AdaBoost with MLP Weak Learners\n",
        "    boost = AdaBoostMLP(n_estimators=10, hidden_size=6, lr=0.02, epochs=150)\n",
        "    boost.fit(X_cv_train, y_cv_train)\n",
        "\n",
        "    # Predict on CV Test Set\n",
        "    y_pred_boost = boost.predict(X_cv_test)\n",
        "    cv_accuracy = accuracy_score(y_cv_test, y_pred_boost)\n",
        "    cv_accuracies.append(cv_accuracy)\n",
        "\n",
        "    # Storing all true and predicted values for full evaluation\n",
        "    all_y_true.extend(y_cv_test)\n",
        "    all_y_pred.extend(y_pred_boost)\n",
        "\n",
        "# Calculating Average CV Accuracy\n",
        "mean_cv_accuracy = np.mean(cv_accuracies)\n",
        "print(f\"\\nCross-Validation AdaBoost-MLP Ensemble (5-Fold)\")\n",
        "print(f\"Mean CV Accuracy: {mean_cv_accuracy:.4f}\")\n",
        "print(classification_report((np.array(all_y_true) == 1).astype(int),\n",
        "                            (np.array(all_y_pred) == 1).astype(int), zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion"
      ],
      "metadata": {
        "id": "RbWjUqgRETuv"
      },
      "id": "RbWjUqgRETuv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially, a single MLP model demonstrated low performance with limited generalization capabilities. However, with the introduction of AdaBoost, the model's efficiency improved significantly. The AdaBoost-MLP ensemble, trained through 5-fold cross-validation, achieved an average accuracy of 91.74%. The model exhibited balanced performance, with class 0 achieving 88% precision and 90% recall, while class 1 reached 94% precision and 93% recall. These results demonstrate that the AdaBoost ensemble effectively transformed weak MLP learners into a strong, robust classifier by adaptively focusing on misclassified samples. The model maintained high recall for class 1, highlighting its ability to correctly identify positive cases while maintaining competitive precision for both classes. The consistently high cross-validation accuracy further confirms the model's stability and strong generalization capability."
      ],
      "metadata": {
        "id": "74LjMGoBEVYq"
      },
      "id": "74LjMGoBEVYq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perceptron‑node Random Forest"
      ],
      "metadata": {
        "id": "Z9wwwjrJ_1IU"
      },
      "id": "Z9wwwjrJ_1IU"
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "b7111542",
      "metadata": {
        "id": "b7111542"
      },
      "outputs": [],
      "source": [
        "class PerceptronNode:\n",
        "    \"\"\"Internal decision node that learns a hyperplane to split the data.\"\"\"\n",
        "    def __init__(self, n_features, lr=0.05, epochs=30):\n",
        "        self.w = np.random.randn(n_features)\n",
        "        self.b = 0.\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def _train(self, X, y):\n",
        "        # Simple perceptron learning rule (PLA) for labels {-1,+1}\n",
        "        for _ in range(self.epochs):\n",
        "            for xi, yi in zip(X, y):\n",
        "                if yi * (np.dot(xi, self.w) + self.b) <= 0:\n",
        "                    self.w += self.lr * yi * xi\n",
        "                    self.b += self.lr * yi\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(X @ self.w + self.b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "f9c466dd",
      "metadata": {
        "id": "f9c466dd"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "class DecisionTreePerceptron:\n",
        "    \"\"\"Binary decision tree whose internal splits are tiny perceptrons.\"\"\"\n",
        "    def __init__(self, max_depth=5, min_samples_leaf=5, max_features=None,\n",
        "                 lr=0.05, epochs=25, rng=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.rng = np.random.default_rng(rng)\n",
        "        self.root = None\n",
        "\n",
        "    class _Node:\n",
        "        __slots__ = ('perceptron', 'left', 'right', 'is_leaf', 'pred')\n",
        "        def __init__(self):\n",
        "            self.perceptron = None\n",
        "            self.left = None\n",
        "            self.right = None\n",
        "            self.is_leaf = False\n",
        "            self.pred = None\n",
        "\n",
        "    def _build(self, X, y, depth):\n",
        "        node = self._Node()\n",
        "\n",
        "        # Stopping conditions\n",
        "        if depth >= self.max_depth or len(X) <= self.min_samples_leaf or len(np.unique(y)) == 1:\n",
        "            node.is_leaf = True\n",
        "            node.pred = 1 if np.sum(y == 1) >= np.sum(y == -1) else -1\n",
        "            return node\n",
        "\n",
        "        n_features_total = X.shape[1]\n",
        "        max_features = self.max_features or int(math.sqrt(n_features_total))\n",
        "        feat_idx = self.rng.choice(n_features_total, size=max_features, replace=False)\n",
        "\n",
        "        # Train perceptron only on selected features\n",
        "        p = PerceptronNode(n_features=max_features, lr=self.lr, epochs=self.epochs)\n",
        "        p._train(X[:, feat_idx], y)\n",
        "\n",
        "        # Split\n",
        "        pred = p.predict(X[:, feat_idx])\n",
        "        left_mask = pred == -1\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        if np.all(left_mask) or np.all(right_mask):  # failed split, make leaf\n",
        "            node.is_leaf = True\n",
        "            node.pred = 1 if np.sum(y == 1) >= np.sum(y == -1) else -1\n",
        "            return node\n",
        "\n",
        "        node.perceptron = (p, feat_idx)\n",
        "        node.left = self._build(X[left_mask], y[left_mask], depth+1)\n",
        "        node.right = self._build(X[right_mask], y[right_mask], depth+1)\n",
        "        return node\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build(X, y, depth=0)\n",
        "\n",
        "    def _predict_one(self, x, node):\n",
        "        while not node.is_leaf:\n",
        "            p, feat_idx = node.perceptron\n",
        "            side = p.predict(x[feat_idx][None, :])[0]\n",
        "            node = node.left if side == -1 else node.right\n",
        "        return node.pred\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_one(x, self.root) for x in X])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "3fe7e2f2",
      "metadata": {
        "id": "3fe7e2f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RandomForestPerceptron:\n",
        "    \"\"\"Ensemble of DecisionTreePerceptron built via bagging.\"\"\"\n",
        "    def __init__(self, n_estimators=25, max_depth=5, min_samples_leaf=5,\n",
        "                 max_features=None, lr=0.05, epochs=25, rng=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.trees = []\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.rng = np.random.default_rng(rng)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        for i in range(self.n_estimators):\n",
        "            # Bootstrap sample\n",
        "            indices = self.rng.choice(n_samples, size=n_samples, replace=True)\n",
        "            X_boot, y_boot = X[indices], y[indices]\n",
        "            tree = DecisionTreePerceptron(max_depth=self.max_depth,\n",
        "                                          min_samples_leaf=self.min_samples_leaf,\n",
        "                                          max_features=self.max_features,\n",
        "                                          lr=self.lr, epochs=self.epochs,\n",
        "                                          rng=self.rng.integers(1e9))\n",
        "            tree.fit(X_boot, y_boot)\n",
        "            self.trees.append(tree)\n",
        "            # print(f\"Tree {i+1}/{self.n_estimators} trained.\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        votes = np.zeros(X.shape[0])\n",
        "        for tree in self.trees:\n",
        "            votes += tree.predict(X)\n",
        "        return np.sign(votes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "799951f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "799951f7",
        "outputId": "9a885a39-ce87-4f11-c912-d62d7e006091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Validation Perceptron Random Forest (5-Fold)\n",
            "Mean CV Accuracy: 0.9631\n",
            "All CV Accuracies: [1.0, 0.9385964912280702, 0.9473684210526315, 0.956140350877193, 0.9734513274336283]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.92      0.95       212\n",
            "           1       0.96      0.99      0.97       357\n",
            "\n",
            "    accuracy                           0.96       569\n",
            "   macro avg       0.97      0.96      0.96       569\n",
            "weighted avg       0.96      0.96      0.96       569\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cross-Validation (5-Fold) Setup\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_accuracies = []\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "\n",
        "# Cross-Validation Loop for Perceptron Random Forest\n",
        "for train_index, test_index in cv.split(X, y_signed):\n",
        "    X_cv_train, X_cv_test = X[train_index], X[test_index]\n",
        "    y_cv_train, y_cv_test = y_signed[train_index], y_signed[test_index]\n",
        "\n",
        "    # Scaling within each fold\n",
        "    scaler = StandardScaler()\n",
        "    X_cv_train = scaler.fit_transform(X_cv_train)\n",
        "    X_cv_test = scaler.transform(X_cv_test)\n",
        "\n",
        "    # Perceptron Random Forest Initialization\n",
        "    forest = RandomForestPerceptron(\n",
        "        n_estimators=25,\n",
        "        max_depth=6,\n",
        "        min_samples_leaf=5,\n",
        "        max_features=int(np.sqrt(X_train.shape[1])),\n",
        "        lr=0.05,\n",
        "        epochs=20,\n",
        "        rng=42\n",
        "    )\n",
        "\n",
        "    # Training the Random Forest on CV Training Data\n",
        "    forest.fit(X_cv_train, y_cv_train)\n",
        "\n",
        "    # Predicting on CV Test Set\n",
        "    y_pred_forest = forest.predict(X_cv_test)\n",
        "\n",
        "    # Calculating and Storing CV Accuracy\n",
        "    cv_accuracy = accuracy_score(y_cv_test, y_pred_forest)\n",
        "    cv_accuracies.append(cv_accuracy)\n",
        "\n",
        "    # Collecting all predictions for full classification report\n",
        "    all_y_true.extend(y_cv_test)\n",
        "    all_y_pred.extend(y_pred_forest)\n",
        "\n",
        "# Calculating Average CV Accuracy\n",
        "mean_cv_accuracy = np.mean(cv_accuracies)\n",
        "\n",
        "print(\"\\nCross-Validation Perceptron Random Forest (5-Fold)\")\n",
        "print(f\"Mean CV Accuracy: {mean_cv_accuracy:.4f}\")\n",
        "print(f\"All CV Accuracies: {cv_accuracies}\")\n",
        "print(classification_report((np.array(all_y_true) == 1).astype(int),\n",
        "                            (np.array(all_y_pred) == 1).astype(int), zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0af1e53e",
      "metadata": {
        "id": "0af1e53e"
      },
      "source": [
        "#Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Perceptron Random Forest model was implemented. In this model, each decision tree within the forest uses perceptrons at its internal nodes to make splitting decisions. Unlike AdaBoost, which focuses on sequentially improving weak learners, the Random Forest approach relies on bagging. Each decision tree (which internally uses perceptrons for splits) is trained independently on a bootstrap sample of the training data. The perceptron at each node of a tree is trained on the subset of data reaching that node and a random subset of features. This promotes model diversity and helps in minimizing errors when the predictions of all trees are aggregated. The resulting Perceptron Random Forest model demonstrated outstanding performance, achieving a cross-validated accuracy of 96.31%. The model maintained high precision and recall for both classes, achieving 98% precision and 92% recall for class 0, and 96% precision and 99% recall for class 1. This balanced performance indicates the model’s ability to correctly classify both classes with minimal errors. These results highlight the robustness of Random Forest in reducing variance through ensemble learning, making it less prone to overfitting compared to AdaBoost. The model’s high accuracy across all cross-validation folds further emphasizes its stability and generalization capability."
      ],
      "metadata": {
        "id": "ydR6G2hREujE"
      },
      "id": "ydR6G2hREujE"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}